{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984e57b3-57bc-45f6-b951-64bcd983974d",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31142458-add0-4695-bcef-9d43cc6f5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f01ffd-9fe5-440d-819f-1f9e62bfe888",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"entregable-2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160c98-d0da-4c69-a900-1bf583481c27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lectura de datos de la API en formato JSON \n",
    "\n",
    "Los datos tienen la siguiente estructura\n",
    "\n",
    "```json\n",
    "[\n",
    "    {dato1},\n",
    "    {dato2},\n",
    "    ...,\n",
    "    {datoN}\n",
    "]\n",
    "```\n",
    "\n",
    "Activamos la opción [multiline](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline) para que Spark pueda armar el DataFrame correctamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6675a081-f0b5-4af0-8f1e-85065b4ba5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(\"api/motorcycles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2c92f-c608-4996-9c90-b152cc492c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploración de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db345701-ab68-42d5-bea4-39a187c50da1",
   "metadata": {},
   "source": [
    "Una posible mejora del entregable 1 era:\n",
    "\n",
    "- El schema puede variar según el fabricante. Podríamos obtener todas las características disponibles en los datos extraídos y definir un schema con todas las columnas posibles para la tabla de Redshift en lugar de priozar un subconjunto de ellas.\n",
    "\n",
    "La tabla creada en la entrega anterior tiene 24 columnas:\n",
    "\n",
    "```\n",
    "1 make\n",
    "2 model\n",
    "3 year\n",
    "4 type\n",
    "5 displacement\n",
    "6 engine\n",
    "7 power\n",
    "8 top_speed\n",
    "9 compression\n",
    "10 bore_stroke\n",
    "11 cooling\n",
    "12 fuel_consumption\n",
    "13 emission\n",
    "14 front_suspension\n",
    "15 rear_suspension\n",
    "16 front_tire\n",
    "17 rear_tire\n",
    "18 front_brakes\n",
    "19 rear_brakes\n",
    "20 dry_weight\n",
    "21 total_height\n",
    "22 total_length\n",
    "23 total_width\n",
    "24 starter\n",
    "```\n",
    "\n",
    "Veamos lo que nos dice Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50098805-7de9-443f-bb06-35e645c0a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# columnas = 41\n"
     ]
    }
   ],
   "source": [
    "df_cols = df.columns\n",
    "\n",
    "print(f\"# columnas = {len(df_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8eaaa-ef87-49fc-8044-35ffde6c461d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Esto nos dice que Spark fue capaz de entender la estructura de los datos desde la carga. El DataFrame tiene un método más cómodo para visualizar el schema completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5c92156-5a2b-4b55-86a2-e610d60594a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bore_stroke: string (nullable = true)\n",
      " |-- clutch: string (nullable = true)\n",
      " |-- compression: string (nullable = true)\n",
      " |-- cooling: string (nullable = true)\n",
      " |-- displacement: string (nullable = true)\n",
      " |-- dry_weight: string (nullable = true)\n",
      " |-- emission: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- frame: string (nullable = true)\n",
      " |-- front_brakes: string (nullable = true)\n",
      " |-- front_suspension: string (nullable = true)\n",
      " |-- front_tire: string (nullable = true)\n",
      " |-- front_wheel_travel: string (nullable = true)\n",
      " |-- fuel_capacity: string (nullable = true)\n",
      " |-- fuel_consumption: string (nullable = true)\n",
      " |-- fuel_control: string (nullable = true)\n",
      " |-- fuel_system: string (nullable = true)\n",
      " |-- gearbox: string (nullable = true)\n",
      " |-- ground_clearance: string (nullable = true)\n",
      " |-- ignition: string (nullable = true)\n",
      " |-- lubrication: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      " |-- rear_brakes: string (nullable = true)\n",
      " |-- rear_suspension: string (nullable = true)\n",
      " |-- rear_tire: string (nullable = true)\n",
      " |-- rear_wheel_travel: string (nullable = true)\n",
      " |-- seat_height: string (nullable = true)\n",
      " |-- starter: string (nullable = true)\n",
      " |-- top_speed: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- total_height: string (nullable = true)\n",
      " |-- total_length: string (nullable = true)\n",
      " |-- total_weight: string (nullable = true)\n",
      " |-- total_width: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- valves_per_cylinder: string (nullable = true)\n",
      " |-- wheelbase: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69f5ac-f6c8-43a3-9444-bf32029395ea",
   "metadata": {},
   "source": [
    "¡Genial! Ahora veamos algunos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7443bb1a-7e53-4310-9647-d68790a4c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "|         bore_stroke|clutch|compression|cooling|        displacement|          dry_weight|            emission|              engine|frame|front_brakes|front_suspension|front_tire|front_wheel_travel|fuel_capacity|    fuel_consumption|fuel_control|fuel_system|gearbox|ground_clearance|ignition|lubrication|   make|            model|               power|         rear_brakes|rear_suspension| rear_tire|rear_wheel_travel|seat_height|        starter|           top_speed|torque|        total_height|        total_length|total_weight|         total_width|transmission| type|valves_per_cylinder|wheelbase|year|\n",
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "|52.4 x 49.5 mm (2...|  null|      8.8:1|    Air|110.0 ccm (6.71 c...|99.0 kg (218.3 po...|48.7 CO2 g/km. (C...|Single cylinder, ...| null| Single disc| Telescopic fork|130/60-13 |              null|         null|2.10 litres/100 k...|        null|       null|   null|            null|    null|       null|Motomel|Blitz 110 Tunning|7.1 HP (5.2  kW))...|Expanding brake (...|   Single shock|130/60-13 |             null|       null|Electric & kick|75.0 km/h (46.6 mph)|  null|1068 mm (42.0 inc...|1900 mm (74.8 inc...|        null|660 mm (26.0 inches)|        null|Sport|               null|     null|2020|\n",
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb941f5-2b63-46cc-a72d-9290e23b34c0",
   "metadata": {},
   "source": [
    "Ilegible. Al parecer es un problema de Jupyter al formatear la tabla que imprime Spark. Probemos jugando con algunos parámetros de [show](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.show.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3220b24c-4f0d-4578-be09-948e76b9d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " bore_stroke         | 52.4 x 49.5 mm (2.1 x 1.9 inches)               \n",
      " clutch              | null                                            \n",
      " compression         | 8.8:1                                           \n",
      " cooling             | Air                                             \n",
      " displacement        | 110.0 ccm (6.71 cubic inches)                   \n",
      " dry_weight          | 99.0 kg (218.3 pounds)                          \n",
      " emission            | 48.7 CO2 g/km. (CO2 - Carbon dioxide emission)  \n",
      " engine              | Single cylinder, four-stroke                    \n",
      " frame               | null                                            \n",
      " front_brakes        | Single disc                                     \n",
      " front_suspension    | Telescopic fork                                 \n",
      " front_tire          | 130/60-13                                       \n",
      " front_wheel_travel  | null                                            \n",
      " fuel_capacity       | null                                            \n",
      " fuel_consumption    | 2.10 litres/100 km (47.6 km/l or 112.01 mpg)    \n",
      " fuel_control        | null                                            \n",
      " fuel_system         | null                                            \n",
      " gearbox             | null                                            \n",
      " ground_clearance    | null                                            \n",
      " ignition            | null                                            \n",
      " lubrication         | null                                            \n",
      " make                | Motomel                                         \n",
      " model               | Blitz 110 Tunning                               \n",
      " power               | 7.1 HP (5.2  kW)) @ 8500 RPM                    \n",
      " rear_brakes         | Expanding brake (drum brake)                    \n",
      " rear_suspension     | Single shock                                    \n",
      " rear_tire           | 130/60-13                                       \n",
      " rear_wheel_travel   | null                                            \n",
      " seat_height         | null                                            \n",
      " starter             | Electric & kick                                 \n",
      " top_speed           | 75.0 km/h (46.6 mph)                            \n",
      " torque              | null                                            \n",
      " total_height        | 1068 mm (42.0 inches)                           \n",
      " total_length        | 1900 mm (74.8 inches)                           \n",
      " total_weight        | null                                            \n",
      " total_width         | 660 mm (26.0 inches)                            \n",
      " transmission        | null                                            \n",
      " type                | Sport                                           \n",
      " valves_per_cylinder | null                                            \n",
      " wheelbase           | null                                            \n",
      " year                | 2020                                            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fa2ce-8088-4792-ab7e-b2de614455cf",
   "metadata": {},
   "source": [
    "Mucho mejor. Vemos que algunas columnas son `null`, lo que tiene sentido porque algunos fabricantes incluyen datos que otros no.\n",
    "\n",
    "Busquemos filas duplicadas. A la cantidad total de filas vamos a restarle la cantidad de filas distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f757adc-52a7-4465-9db5-e3140bbccee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trs = 750\n"
     ]
    }
   ],
   "source": [
    "total_rows = df.select(df_cols).count()\n",
    "print(f\"trs = {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36d4344-7c02-4c72-875b-a581dd726519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdrs = 750\n"
     ]
    }
   ],
   "source": [
    "total_distinct_rows = df.select(df_cols).distinct().count()\n",
    "print(f\"tdrs = {total_distinct_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c351f88c-7948-4856-a0c0-4b7374f79a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated rows: trs - tdrs = 0\n"
     ]
    }
   ],
   "source": [
    "repeated_rows = total_rows - total_distinct_rows\n",
    "print(f\"Repeated rows: trs - tdrs = {repeated_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576d89-a425-4bf9-9dac-6c7e5f1448e1",
   "metadata": {},
   "source": [
    "Por lo que no tenemos datos repetidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0b5df-58b3-4738-ae1a-d5db14e01f30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13226b-324a-40c0-b61b-2bafd686b9d1",
   "metadata": {},
   "source": [
    "### Año\n",
    "\n",
    "Si volvemos al schema del DataFrame vamos a notar que la columna `year` es de tipo `string`. Hagamos, por conveniencia, que sea de tipo `integer`. Para eso vamos a usar la función `col` de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc878525-87ec-469c-a03f-a0e8e81e727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "IntegerType()\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "transformed_df = df.withColumn(\"year\", col(\"year\").cast(\"Integer\"))\n",
    "print(df.schema[\"year\"].dataType)\n",
    "print(transformed_df.schema[\"year\"].dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89954ce-1b45-4dce-8d13-08cd8308c3d0",
   "metadata": {},
   "source": [
    "El nuevo DataFrame `transformed_df` tiene el mismo schema que el DataFrame original salvo por la columna `year` que ahora es un `integer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bfeb8-82ea-4012-9eb5-c794810658a0",
   "metadata": {},
   "source": [
    "### Peso seco y total\n",
    "\n",
    "Si volvemos al schema, vamos a notar que algunos fabricantes dan el `dry_weight`(peso de la moto sin fluidos como combustible, refrigerante, etc) y que otros dan el `total_weight`. En ambos casos se trata de strings. Nuestro objetivo consiste en crear dos columnas nuevas `dry_weight_kg` y `total_weight_kg` de tipo `float` en nuestro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb33d3bc-e3af-4733-a079-c3cd5c809832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "StringType()\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df.schema[\"dry_weight\"].dataType)\n",
    "print(transformed_df.schema[\"total_weight\"].dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9d720-6f80-4097-bc0f-c1a52c4d2f73",
   "metadata": {},
   "source": [
    "Ahora veamos el formato de estas strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19189b12-68d9-4964-a1a6-0664c41e4180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.0 kg (218.3 pounds)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como tenemos pocos datos podemos aprovechar collect.\n",
    "rows = transformed_df.collect() \n",
    "rows[0].dry_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d61c2f-0e60-4c15-a776-d865cbd8e602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'415.9 kg (917.0 pounds)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[749].total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8367c4e9-444a-4a23-b1d1-933b9edb0459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"99.0 kg (218.3 pounds)\".split(\" kg\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f68dfc-edf3-48fc-9f73-695f5e945ff6",
   "metadata": {},
   "source": [
    "Vamos a aprovechar las [UDFs](https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/#pyspark-udf-withcolumn) de PySpark para transformar las columnas. Primero extraemos el valor en kg de la string y luego lo convertimos a `float`. Si es `NULL` devolvemos ese mismo valor sin modificación alguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcc9488a-3365-478d-8c5d-17384d016a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bore_stroke: string (nullable = true)\n",
      " |-- clutch: string (nullable = true)\n",
      " |-- compression: string (nullable = true)\n",
      " |-- cooling: string (nullable = true)\n",
      " |-- displacement: string (nullable = true)\n",
      " |-- dry_weight: string (nullable = true)\n",
      " |-- emission: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- frame: string (nullable = true)\n",
      " |-- front_brakes: string (nullable = true)\n",
      " |-- front_suspension: string (nullable = true)\n",
      " |-- front_tire: string (nullable = true)\n",
      " |-- front_wheel_travel: string (nullable = true)\n",
      " |-- fuel_capacity: string (nullable = true)\n",
      " |-- fuel_consumption: string (nullable = true)\n",
      " |-- fuel_control: string (nullable = true)\n",
      " |-- fuel_system: string (nullable = true)\n",
      " |-- gearbox: string (nullable = true)\n",
      " |-- ground_clearance: string (nullable = true)\n",
      " |-- ignition: string (nullable = true)\n",
      " |-- lubrication: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      " |-- rear_brakes: string (nullable = true)\n",
      " |-- rear_suspension: string (nullable = true)\n",
      " |-- rear_tire: string (nullable = true)\n",
      " |-- rear_wheel_travel: string (nullable = true)\n",
      " |-- seat_height: string (nullable = true)\n",
      " |-- starter: string (nullable = true)\n",
      " |-- top_speed: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- total_height: string (nullable = true)\n",
      " |-- total_length: string (nullable = true)\n",
      " |-- total_weight: string (nullable = true)\n",
      " |-- total_width: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- valves_per_cylinder: string (nullable = true)\n",
      " |-- wheelbase: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- dry_weight_kg: float (nullable = true)\n",
      " |-- total_weight_kg: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def weight_in_kg(value):\n",
    "    if value:\n",
    "        return float(value.split(\" kg\")[0])\n",
    "    return None\n",
    "\n",
    "udf_weight_in_kg = F.udf(weight_in_kg, FloatType())\n",
    "for column in [\"dry_weight\", \"total_weight\"]:\n",
    "    transformed_df = transformed_df.withColumn(f\"{column}_kg\",  udf_weight_in_kg(column))\n",
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b2bfb-2c7e-44f5-800b-16fc92b4ecc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Abusemos de `collect` una vez más para explorar el resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83595103-ddb4-442d-8d99-ad4e58e44b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = transformed_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "823892a1-f70e-4434-ac79-2f5f6ec17eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry_weight = 99.0 kg (218.3 pounds) => dry_weight_kg = 99.0\n",
      "total_weight = None => total_weight_kg = None\n",
      "dry_weight = 247.0 kg (544.5 pounds) => dry_weight_kg = 247.0\n",
      "total_weight = 252.0 kg (555.6 pounds) => total_weight_kg = 252.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"dry_weight = {rows[0].dry_weight} => dry_weight_kg = {rows[0].dry_weight_kg}\")\n",
    "print(f\"total_weight = {rows[1].total_weight} => total_weight_kg = {rows[1].total_weight_kg}\")\n",
    "print(f\"dry_weight = {rows[555].dry_weight} => dry_weight_kg = {rows[555].dry_weight_kg}\")\n",
    "print(f\"total_weight = {rows[555].total_weight} => total_weight_kg = {rows[555].total_weight_kg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5915d-b736-439e-a358-a237688f258b",
   "metadata": {},
   "source": [
    "¡Nuestra transformación tiene buena pinta!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cacde-f3a8-479d-ab97-485ed9f3842b",
   "metadata": {},
   "source": [
    "# Carga en Redshift\n",
    "\n",
    "Vamos a usar `redshift-connector` para crear una nueva tabla `motorcycles2` en Redshift, luego vamos a cargar los datos con Spark. Instalamos `redshift-connector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b969a605-920f-411c-a12d-267c60fbdd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redshift-connector in /opt/conda/lib/python3.10/site-packages (2.0.911)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (2022.5)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.12.201 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (1.29.152)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (65.5.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (4.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (2.28.1)\n",
      "Requirement already satisfied: lxml>=4.6.5 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (4.9.2)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.9.201 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (1.26.152)\n",
      "Requirement already satisfied: scramp<1.5.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (1.4.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector) (2.3.2.post1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift-connector) (0.6.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.9.201->redshift-connector) (1.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift-connector) (1.26.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift-connector) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (2022.9.24)\n",
      "Requirement already satisfied: asn1crypto>=1.5.1 in /opt/conda/lib/python3.10/site-packages (from scramp<1.5.0,>=1.2.0->redshift-connector) (1.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->redshift-connector) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift-connector) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install redshift-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d674141-0448-4436-bee0-aa9d034d68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "from os import environ\n",
    "\n",
    "connection = redshift_connector.connect(\n",
    "    host=environ[\"REDSHIFT_CODER_HOST\"],\n",
    "    database=environ[\"REDSHIFT_CODER_DB\"],\n",
    "    port=int(environ[\"REDSHIFT_CODER_PORT\"]),\n",
    "    user=environ[\"REDSHIFT_CODER_USER\"],\n",
    "    password=environ[\"REDSHIFT_CODER_PASSWORD\"])\n",
    "connection.autocommit = True\n",
    "cursor = connection.cursor()\n",
    "\n",
    "table_name = \"motorcycles2\"\n",
    "schema_table = f\"{environ['REDSHIFT_CODER_SCHEMA']}.{table_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50652901-ec97-46a9-9538-d85c0f50ca47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x7fb818e19cc0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podría haber aprovechado transformed_df.schema.json() para crear la tabla.\n",
    "statement = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {schema_table}(\n",
    "    make varchar not null,\n",
    "    model varchar not null,\n",
    "    year integer not null,\n",
    "    type varchar not null,\n",
    "    bore_stroke varchar,\n",
    "    clutch varchar,\n",
    "    compression varchar,\n",
    "    cooling varchar,\n",
    "    displacement varchar,\n",
    "    dry_weight varchar,\n",
    "    emission varchar,\n",
    "    engine varchar,\n",
    "    frame varchar,\n",
    "    front_brakes varchar,\n",
    "    front_suspension varchar,\n",
    "    front_tire varchar,\n",
    "    front_wheel_travel varchar,\n",
    "    fuel_capacity varchar,\n",
    "    fuel_consumption varchar,\n",
    "    fuel_control varchar,\n",
    "    fuel_system varchar,\n",
    "    gearbox varchar,\n",
    "    ground_clearance varchar,\n",
    "    ignition varchar,\n",
    "    lubrication varchar,\n",
    "    power varchar,\n",
    "    rear_brakes varchar,\n",
    "    rear_suspension varchar,\n",
    "    rear_tire varchar,\n",
    "    rear_wheel_travel varchar,\n",
    "    seat_height varchar,\n",
    "    starter varchar,\n",
    "    top_speed varchar,\n",
    "    torque varchar,\n",
    "    total_height varchar,\n",
    "    total_length varchar,\n",
    "    total_weight varchar,\n",
    "    total_width varchar,\n",
    "    transmission varchar,\n",
    "    valves_per_cylinder varchar,\n",
    "    wheelbase varchar,\n",
    "    dry_weight_kg float,\n",
    "    total_weight_kg float\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec22ae9-3d12-4feb-a7b1-5234d83e345b",
   "metadata": {},
   "source": [
    "Con la tabla lista, ya podemos pasar a cargar los datos con Spark. Nota: le lleva un tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eba31065-e65a-454f-9b2c-70970700714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.65 ms, sys: 140 µs, total: 9.79 ms\n",
      "Wall time: 49.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r = transformed_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{environ['REDSHIFT_CODER_HOST']}:{environ['REDSHIFT_CODER_PORT']}/{environ['REDSHIFT_CODER_DB']}\") \\\n",
    "    .option(\"dbtable\", schema_table) \\\n",
    "    .option(\"user\", environ['REDSHIFT_CODER_USER']) \\\n",
    "    .option(\"password\", environ['REDSHIFT_CODER_PASSWORD']) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1b0bf-05d0-48ee-aae5-051c0c0fef18",
   "metadata": {},
   "source": [
    "Veamos si podemos leer los datos cargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e5a7ab8-c71f-429b-8d12-3ee9cedb21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 750 entries, 0 to 749\n",
      "Data columns (total 43 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   bore_stroke          712 non-null    object \n",
      " 1   clutch               501 non-null    object \n",
      " 2   compression          697 non-null    object \n",
      " 3   cooling              743 non-null    object \n",
      " 4   displacement         747 non-null    object \n",
      " 5   dry_weight           352 non-null    object \n",
      " 6   emission             396 non-null    object \n",
      " 7   engine               750 non-null    object \n",
      " 8   frame                430 non-null    object \n",
      " 9   front_brakes         749 non-null    object \n",
      " 10  front_suspension     704 non-null    object \n",
      " 11  front_tire           736 non-null    object \n",
      " 12  front_wheel_travel   345 non-null    object \n",
      " 13  fuel_capacity        728 non-null    object \n",
      " 14  fuel_consumption     396 non-null    object \n",
      " 15  fuel_control         599 non-null    object \n",
      " 16  fuel_system          708 non-null    object \n",
      " 17  gearbox              690 non-null    object \n",
      " 18  ground_clearance     625 non-null    object \n",
      " 19  ignition             411 non-null    object \n",
      " 20  lubrication          230 non-null    object \n",
      " 21  make                 750 non-null    object \n",
      " 22  model                750 non-null    object \n",
      " 23  power                562 non-null    object \n",
      " 24  rear_brakes          748 non-null    object \n",
      " 25  rear_suspension      692 non-null    object \n",
      " 26  rear_tire            735 non-null    object \n",
      " 27  rear_wheel_travel    343 non-null    object \n",
      " 28  seat_height          685 non-null    object \n",
      " 29  starter              716 non-null    object \n",
      " 30  top_speed            56 non-null     object \n",
      " 31  torque               560 non-null    object \n",
      " 32  total_height         439 non-null    object \n",
      " 33  total_length         709 non-null    object \n",
      " 34  total_weight         629 non-null    object \n",
      " 35  total_width          459 non-null    object \n",
      " 36  transmission         716 non-null    object \n",
      " 37  type                 750 non-null    object \n",
      " 38  valves_per_cylinder  332 non-null    object \n",
      " 39  wheelbase            688 non-null    object \n",
      " 40  year                 750 non-null    int64  \n",
      " 41  dry_weight_kg        352 non-null    float64\n",
      " 42  total_weight_kg      629 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(40)\n",
      "memory usage: 252.1+ KB\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(f\"select * from {schema_table}\")\n",
    "df = cursor.fetch_dataframe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a69bc4-5021-4ce3-9f24-2b928b015cb0",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "- Como vimos en clase, trabajar con Spark es muy similar, para cosas simples, a Pandas.\n",
    "- Los puntos de mayor sufrimiento se dieron en la lectura y en la carga del DataFrame. La documentación de Spark no me resultó clara y tampoco parece haber una sola manera de pasar los .jar que necesita para conectarse a Redshift(que tampoco tiene ejemplos claro para esto). Fue prueba y error hasta que salió."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77a80e-005e-455a-9ae5-20014323a6e9",
   "metadata": {},
   "source": [
    "# Posibles mejoras\n",
    "\n",
    "- Podría haber leído los datos de la API desde Redshift utilizando la tabla del primer entregable.\n",
    "- Hay muchos datos que comparten la misma estructura: una string con el dato en distintas unidades. Podría haber hecho lo mismo que hice con el peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb52367-db5c-4184-afa0-8f88e257cc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
