{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5070c9f-6c70-4d6c-99fe-0a4c53ed886d",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Load in new Redshift table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e57b3-57bc-45f6-b951-64bcd983974d",
   "metadata": {},
   "source": [
    "## Spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31142458-add0-4695-bcef-9d43cc6f5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f01ffd-9fe5-440d-819f-1f9e62bfe888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"entregable-2\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "     # .config(\"spark.jars\", driver_path) \\\n",
    "        # .config(\"spark.executor.extraClassPath\", driver_path) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f377a8b-5e77-4d98-8ba3-e00e6b24306a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.driver.port', '37787'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://6ad3cba5f73b:37787/jars/RedshiftJDBC42-no-awssdk-1.2.36.1060.jar,spark://6ad3cba5f73b:37787/jars/postgresql-42.6.0.jar,spark://6ad3cba5f73b:37787/jars/redshift-jdbc42-2.1.0.16.jar'),\n",
       " ('spark.app.submitTime', '1686633708706'),\n",
       " ('spark.app.id', 'local-1686633709742'),\n",
       " ('spark.jars',\n",
       "  'file:///home/entregable_2/driver_jdbc/RedshiftJDBC42-no-awssdk-1.2.36.1060.jar,file:///home/entregable_2/driver_jdbc/postgresql-42.6.0.jar,file:///home/entregable_2/driver_jdbc/redshift-jdbc42-2.1.0.16.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.name', 'entregable-2'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.startTime', '1686633708948'),\n",
       " ('spark.driver.host', '6ad3cba5f73b'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/entregable_2/driver_jdbc/RedshiftJDBC42-no-awssdk-1.2.36.1060.jar,file:///home/entregable_2/driver_jdbc/postgresql-42.6.0.jar,file:///home/entregable_2/driver_jdbc/redshift-jdbc42-2.1.0.16.jar')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38160c98-d0da-4c69-a900-1bf583481c27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lectura de datos de la API en formato JSON \n",
    "\n",
    "Los datos tienen la siguiente estructura\n",
    "\n",
    "```json\n",
    "[\n",
    "    {dato1},\n",
    "    {dato2},\n",
    "    ...,\n",
    "    {datoN}\n",
    "]\n",
    "```\n",
    "\n",
    "Activamos la opción [multiline](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/#read-json-multiline) para que Spark pueda armar el DataFrame correctamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6675a081-f0b5-4af0-8f1e-85065b4ba5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\", \"true\").json(\"api/motorcycles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2c92f-c608-4996-9c90-b152cc492c41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploración de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db345701-ab68-42d5-bea4-39a187c50da1",
   "metadata": {},
   "source": [
    "Una posible mejora del entregable 1 era:\n",
    "\n",
    "- El schema puede variar según el fabricante. Podríamos obtener todas las características disponibles en los datos extraídos y definir un schema con todas las columnas posibles para la tabla de Redshift en lugar de priozar un subconjunto de ellas.\n",
    "\n",
    "La tabla creada en la entrega anterior tiene 24 columnas:\n",
    "\n",
    "```\n",
    "1 make\n",
    "2 model\n",
    "3 year\n",
    "4 type\n",
    "5 displacement\n",
    "6 engine\n",
    "7 power\n",
    "8 top_speed\n",
    "9 compression\n",
    "10 bore_stroke\n",
    "11 cooling\n",
    "12 fuel_consumption\n",
    "13 emission\n",
    "14 front_suspension\n",
    "15 rear_suspension\n",
    "16 front_tire\n",
    "17 rear_tire\n",
    "18 front_brakes\n",
    "19 rear_brakes\n",
    "20 dry_weight\n",
    "21 total_height\n",
    "22 total_length\n",
    "23 total_width\n",
    "24 starter\n",
    "```\n",
    "\n",
    "Veamos lo que nos dice Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50098805-7de9-443f-bb06-35e645c0a24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# columnas = 41\n"
     ]
    }
   ],
   "source": [
    "df_cols = df.columns\n",
    "\n",
    "print(f\"# columnas = {len(df_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8eaaa-ef87-49fc-8044-35ffde6c461d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Esto nos dice que Spark fue capaz de entender la estructura de los datos desde la carga. El DataFrame tiene un método más cómodo para visualizar el schema completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c92156-5a2b-4b55-86a2-e610d60594a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bore_stroke: string (nullable = true)\n",
      " |-- clutch: string (nullable = true)\n",
      " |-- compression: string (nullable = true)\n",
      " |-- cooling: string (nullable = true)\n",
      " |-- displacement: string (nullable = true)\n",
      " |-- dry_weight: string (nullable = true)\n",
      " |-- emission: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- frame: string (nullable = true)\n",
      " |-- front_brakes: string (nullable = true)\n",
      " |-- front_suspension: string (nullable = true)\n",
      " |-- front_tire: string (nullable = true)\n",
      " |-- front_wheel_travel: string (nullable = true)\n",
      " |-- fuel_capacity: string (nullable = true)\n",
      " |-- fuel_consumption: string (nullable = true)\n",
      " |-- fuel_control: string (nullable = true)\n",
      " |-- fuel_system: string (nullable = true)\n",
      " |-- gearbox: string (nullable = true)\n",
      " |-- ground_clearance: string (nullable = true)\n",
      " |-- ignition: string (nullable = true)\n",
      " |-- lubrication: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      " |-- rear_brakes: string (nullable = true)\n",
      " |-- rear_suspension: string (nullable = true)\n",
      " |-- rear_tire: string (nullable = true)\n",
      " |-- rear_wheel_travel: string (nullable = true)\n",
      " |-- seat_height: string (nullable = true)\n",
      " |-- starter: string (nullable = true)\n",
      " |-- top_speed: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- total_height: string (nullable = true)\n",
      " |-- total_length: string (nullable = true)\n",
      " |-- total_weight: string (nullable = true)\n",
      " |-- total_width: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- valves_per_cylinder: string (nullable = true)\n",
      " |-- wheelbase: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69f5ac-f6c8-43a3-9444-bf32029395ea",
   "metadata": {},
   "source": [
    "¡Genial! Ahora veamos algunos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7443bb1a-7e53-4310-9647-d68790a4c4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "|         bore_stroke|clutch|compression|cooling|        displacement|          dry_weight|            emission|              engine|frame|front_brakes|front_suspension|front_tire|front_wheel_travel|fuel_capacity|    fuel_consumption|fuel_control|fuel_system|gearbox|ground_clearance|ignition|lubrication|   make|            model|               power|         rear_brakes|rear_suspension| rear_tire|rear_wheel_travel|seat_height|        starter|           top_speed|torque|        total_height|        total_length|total_weight|         total_width|transmission| type|valves_per_cylinder|wheelbase|year|\n",
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "|52.4 x 49.5 mm (2...|  null|      8.8:1|    Air|110.0 ccm (6.71 c...|99.0 kg (218.3 po...|48.7 CO2 g/km. (C...|Single cylinder, ...| null| Single disc| Telescopic fork|130/60-13 |              null|         null|2.10 litres/100 k...|        null|       null|   null|            null|    null|       null|Motomel|Blitz 110 Tunning|7.1 HP (5.2  kW))...|Expanding brake (...|   Single shock|130/60-13 |             null|       null|Electric & kick|75.0 km/h (46.6 mph)|  null|1068 mm (42.0 inc...|1900 mm (74.8 inc...|        null|660 mm (26.0 inches)|        null|Sport|               null|     null|2020|\n",
      "+--------------------+------+-----------+-------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------------+----------+------------------+-------------+--------------------+------------+-----------+-------+----------------+--------+-----------+-------+-----------------+--------------------+--------------------+---------------+----------+-----------------+-----------+---------------+--------------------+------+--------------------+--------------------+------------+--------------------+------------+-----+-------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb941f5-2b63-46cc-a72d-9290e23b34c0",
   "metadata": {},
   "source": [
    "Ilegible. Al parecer es un problema de Jupyter al formatear la tabla que imprime Spark. Probemos jugando con algunos parámetros de [show](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.show.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3220b24c-4f0d-4578-be09-948e76b9d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " bore_stroke         | 52.4 x 49.5 mm (2.1 x 1.9 inches)               \n",
      " clutch              | null                                            \n",
      " compression         | 8.8:1                                           \n",
      " cooling             | Air                                             \n",
      " displacement        | 110.0 ccm (6.71 cubic inches)                   \n",
      " dry_weight          | 99.0 kg (218.3 pounds)                          \n",
      " emission            | 48.7 CO2 g/km. (CO2 - Carbon dioxide emission)  \n",
      " engine              | Single cylinder, four-stroke                    \n",
      " frame               | null                                            \n",
      " front_brakes        | Single disc                                     \n",
      " front_suspension    | Telescopic fork                                 \n",
      " front_tire          | 130/60-13                                       \n",
      " front_wheel_travel  | null                                            \n",
      " fuel_capacity       | null                                            \n",
      " fuel_consumption    | 2.10 litres/100 km (47.6 km/l or 112.01 mpg)    \n",
      " fuel_control        | null                                            \n",
      " fuel_system         | null                                            \n",
      " gearbox             | null                                            \n",
      " ground_clearance    | null                                            \n",
      " ignition            | null                                            \n",
      " lubrication         | null                                            \n",
      " make                | Motomel                                         \n",
      " model               | Blitz 110 Tunning                               \n",
      " power               | 7.1 HP (5.2  kW)) @ 8500 RPM                    \n",
      " rear_brakes         | Expanding brake (drum brake)                    \n",
      " rear_suspension     | Single shock                                    \n",
      " rear_tire           | 130/60-13                                       \n",
      " rear_wheel_travel   | null                                            \n",
      " seat_height         | null                                            \n",
      " starter             | Electric & kick                                 \n",
      " top_speed           | 75.0 km/h (46.6 mph)                            \n",
      " torque              | null                                            \n",
      " total_height        | 1068 mm (42.0 inches)                           \n",
      " total_length        | 1900 mm (74.8 inches)                           \n",
      " total_weight        | null                                            \n",
      " total_width         | 660 mm (26.0 inches)                            \n",
      " transmission        | null                                            \n",
      " type                | Sport                                           \n",
      " valves_per_cylinder | null                                            \n",
      " wheelbase           | null                                            \n",
      " year                | 2020                                            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fa2ce-8088-4792-ab7e-b2de614455cf",
   "metadata": {},
   "source": [
    "Mucho mejor. Vemos que algunas columnas son `null`, lo que tiene sentido porque algunos fabricantes incluyen datos que otros no.\n",
    "\n",
    "Busquemos filas duplicadas. A la cantidad total de filas vamos a restarle la cantidad de filas distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f757adc-52a7-4465-9db5-e3140bbccee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trs = 750\n"
     ]
    }
   ],
   "source": [
    "total_rows = df.select(df_cols).count()\n",
    "print(f\"trs = {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a36d4344-7c02-4c72-875b-a581dd726519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tdrs = 750\n"
     ]
    }
   ],
   "source": [
    "total_distinct_rows = df.select(df_cols).distinct().count()\n",
    "print(f\"tdrs = {total_distinct_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c351f88c-7948-4856-a0c0-4b7374f79a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated rows: trs - tdrs = 0\n"
     ]
    }
   ],
   "source": [
    "repeated_rows = total_rows - total_distinct_rows\n",
    "print(f\"Repeated rows: trs - tdrs = {repeated_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576d89-a425-4bf9-9dac-6c7e5f1448e1",
   "metadata": {},
   "source": [
    "Por lo que no tenemos datos repetidos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0b5df-58b3-4738-ae1a-d5db14e01f30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13226b-324a-40c0-b61b-2bafd686b9d1",
   "metadata": {},
   "source": [
    "### Año\n",
    "\n",
    "Si volvemos al schema del DataFrame vamos a notar que la columna `year` es de tipo `string`. Hagamos, por conveniencia, que sea de tipo `integer`. Para eso vamos a usar la función `col` de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc878525-87ec-469c-a03f-a0e8e81e727f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "IntegerType()\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "transformed_df = df.withColumn(\"year\", col(\"year\").cast(\"Integer\"))\n",
    "print(df.schema[\"year\"].dataType)\n",
    "print(transformed_df.schema[\"year\"].dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89954ce-1b45-4dce-8d13-08cd8308c3d0",
   "metadata": {},
   "source": [
    "El nuevo DataFrame `transformed_df` tiene el mismo schema que el DataFrame original salvo por la columna `year` que ahora es un `integer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bfeb8-82ea-4012-9eb5-c794810658a0",
   "metadata": {},
   "source": [
    "### Peso seco y total\n",
    "\n",
    "Si volvemos al schema, vamos a notar que algunos fabricantes dan el `dry_weight`(peso de la moto sin fluidos como combustible, refrigerante, etc) y que otros dan el `total_weight`. En ambos casos se trata de strings. Nuestro objetivo consiste en crear dos columnas nuevas `dry_weight_kg` y `total_weight_kg` de tipo `float` en nuestro DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb33d3bc-e3af-4733-a079-c3cd5c809832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType()\n",
      "StringType()\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df.schema[\"dry_weight\"].dataType)\n",
    "print(transformed_df.schema[\"total_weight\"].dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b9d720-6f80-4097-bc0f-c1a52c4d2f73",
   "metadata": {},
   "source": [
    "Ahora veamos el formato de estas strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19189b12-68d9-4964-a1a6-0664c41e4180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.0 kg (218.3 pounds)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como tenemos pocos datos podemos aprovechar collect.\n",
    "rows = transformed_df.collect() \n",
    "rows[0].dry_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d61c2f-0e60-4c15-a776-d865cbd8e602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'415.9 kg (917.0 pounds)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[749].total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8367c4e9-444a-4a23-b1d1-933b9edb0459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"99.0 kg (218.3 pounds)\".split(\" kg\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f68dfc-edf3-48fc-9f73-695f5e945ff6",
   "metadata": {},
   "source": [
    "Vamos a aprovechar las [UDFs](https://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/#pyspark-udf-withcolumn) de PySpark para transformar las columnas. Primero extraemos el valor en kg de la string y luego lo convertimos a `float`. Si es `NULL` devolvemos ese mismo valor sin modificación alguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcc9488a-3365-478d-8c5d-17384d016a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bore_stroke: string (nullable = true)\n",
      " |-- clutch: string (nullable = true)\n",
      " |-- compression: string (nullable = true)\n",
      " |-- cooling: string (nullable = true)\n",
      " |-- displacement: string (nullable = true)\n",
      " |-- dry_weight: string (nullable = true)\n",
      " |-- emission: string (nullable = true)\n",
      " |-- engine: string (nullable = true)\n",
      " |-- frame: string (nullable = true)\n",
      " |-- front_brakes: string (nullable = true)\n",
      " |-- front_suspension: string (nullable = true)\n",
      " |-- front_tire: string (nullable = true)\n",
      " |-- front_wheel_travel: string (nullable = true)\n",
      " |-- fuel_capacity: string (nullable = true)\n",
      " |-- fuel_consumption: string (nullable = true)\n",
      " |-- fuel_control: string (nullable = true)\n",
      " |-- fuel_system: string (nullable = true)\n",
      " |-- gearbox: string (nullable = true)\n",
      " |-- ground_clearance: string (nullable = true)\n",
      " |-- ignition: string (nullable = true)\n",
      " |-- lubrication: string (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- power: string (nullable = true)\n",
      " |-- rear_brakes: string (nullable = true)\n",
      " |-- rear_suspension: string (nullable = true)\n",
      " |-- rear_tire: string (nullable = true)\n",
      " |-- rear_wheel_travel: string (nullable = true)\n",
      " |-- seat_height: string (nullable = true)\n",
      " |-- starter: string (nullable = true)\n",
      " |-- top_speed: string (nullable = true)\n",
      " |-- torque: string (nullable = true)\n",
      " |-- total_height: string (nullable = true)\n",
      " |-- total_length: string (nullable = true)\n",
      " |-- total_weight: string (nullable = true)\n",
      " |-- total_width: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- valves_per_cylinder: string (nullable = true)\n",
      " |-- wheelbase: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- dry_weight_kg: float (nullable = true)\n",
      " |-- total_weight_kg: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def weight_in_kg(value):\n",
    "    if value:\n",
    "        return float(value.split(\" kg\")[0])\n",
    "    return None\n",
    "\n",
    "udf_weight_in_kg = F.udf(weight_in_kg, FloatType())\n",
    "for column in [\"dry_weight\", \"total_weight\"]:\n",
    "    transformed_df = transformed_df.withColumn(f\"{column}_kg\",  udf_weight_in_kg(column))\n",
    "transformed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b2bfb-2c7e-44f5-800b-16fc92b4ecc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Abusemos de `collect` una vez más para explorar el resultado final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83595103-ddb4-442d-8d99-ad4e58e44b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = transformed_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "823892a1-f70e-4434-ac79-2f5f6ec17eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry_weight = 99.0 kg (218.3 pounds) => dry_weight_kg = 99.0\n",
      "total_weight = None => total_weight_kg = None\n",
      "dry_weight = 247.0 kg (544.5 pounds) => dry_weight_kg = 247.0\n",
      "total_weight = 252.0 kg (555.6 pounds) => total_weight_kg = 252.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"dry_weight = {rows[0].dry_weight} => dry_weight_kg = {rows[0].dry_weight_kg}\")\n",
    "print(f\"total_weight = {rows[1].total_weight} => total_weight_kg = {rows[1].total_weight_kg}\")\n",
    "print(f\"dry_weight = {rows[555].dry_weight} => dry_weight_kg = {rows[555].dry_weight_kg}\")\n",
    "print(f\"total_weight = {rows[555].total_weight} => total_weight_kg = {rows[555].total_weight_kg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c5915d-b736-439e-a358-a237688f258b",
   "metadata": {},
   "source": [
    "¡Nuestra transformación tiene buena pinta!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3cacde-f3a8-479d-ab97-485ed9f3842b",
   "metadata": {},
   "source": [
    "# Carga en Redshift\n",
    "\n",
    "Vamos a usar `redshift-connector` para crear una nueva tabla `motorcycles2` en Redshift, luego vamos a cargar los datos con Spark. Instalamos `redshift-connector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b969a605-920f-411c-a12d-267c60fbdd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redshift-connector\n",
      "  Downloading redshift_connector-2.0.911-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting boto3<2.0.0,>=1.9.201\n",
      "  Downloading boto3-1.26.152-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (65.5.0)\n",
      "Collecting lxml>=4.6.5\n",
      "  Downloading lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore<2.0.0,>=1.12.201\n",
      "  Downloading botocore-1.29.152-py3-none-any.whl (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (2022.5)\n",
      "Collecting scramp<1.5.0,>=1.2.0\n",
      "  Downloading scramp-1.4.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (4.11.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from redshift-connector) (2.28.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector) (2.3.2.post1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift-connector) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.12.201->redshift-connector) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->redshift-connector) (3.4)\n",
      "Collecting asn1crypto>=1.5.1\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->redshift-connector) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.12.201->redshift-connector) (1.16.0)\n",
      "Installing collected packages: asn1crypto, scramp, lxml, jmespath, botocore, s3transfer, boto3, redshift-connector\n",
      "Successfully installed asn1crypto-1.5.1 boto3-1.26.152 botocore-1.29.152 jmespath-1.0.1 lxml-4.9.2 redshift-connector-2.0.911 s3transfer-0.6.1 scramp-1.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install redshift-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d674141-0448-4436-bee0-aa9d034d68eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "from os import environ\n",
    "\n",
    "connection = redshift_connector.connect(\n",
    "    host=environ[\"REDSHIFT_CODER_HOST\"],\n",
    "    database=environ[\"REDSHIFT_CODER_DB\"],\n",
    "    port=int(environ[\"REDSHIFT_CODER_PORT\"]),\n",
    "    user=environ[\"REDSHIFT_CODER_USER\"],\n",
    "    password=environ[\"REDSHIFT_CODER_PASSWORD\"])\n",
    "connection.autocommit = True\n",
    "cursor = connection.cursor()\n",
    "\n",
    "table_name = \"motorcycles2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50652901-ec97-46a9-9538-d85c0f50ca47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<redshift_connector.cursor.Cursor at 0x7fb436cf2c20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podría haber aprovechado transformed_df.schema.json() para crear la tabla.\n",
    "statement = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {environ['REDSHIFT_CODER_SCHEMA']}.{table_name} (\n",
    "    make varchar not null,\n",
    "    model varchar not null,\n",
    "    year integer not null,\n",
    "    type varchar not null,\n",
    "    bore_stroke varchar,\n",
    "    clutch varchar,\n",
    "    compression varchar,\n",
    "    cooling varchar,\n",
    "    displacement varchar,\n",
    "    dry_weight varchar,\n",
    "    emission varchar,\n",
    "    engine varchar,\n",
    "    frame varchar,\n",
    "    front_brakes varchar,\n",
    "    front_suspension varchar,\n",
    "    front_tire varchar,\n",
    "    front_wheel_travel varchar,\n",
    "    fuel_capacity varchar,\n",
    "    fuel_consumption varchar,\n",
    "    fuel_control varchar,\n",
    "    fuel_system varchar,\n",
    "    gearbox varchar,\n",
    "    ground_clearance varchar,\n",
    "    ignition varchar,\n",
    "    lubrication varchar,\n",
    "    power varchar,\n",
    "    rear_brakes varchar,\n",
    "    rear_suspension varchar,\n",
    "    rear_tire varchar,\n",
    "    rear_wheel_travel varchar,\n",
    "    seat_height varchar,\n",
    "    starter varchar,\n",
    "    top_speed varchar,\n",
    "    torque varchar,\n",
    "    total_height varchar,\n",
    "    total_length varchar,\n",
    "    total_weight varchar,\n",
    "    total_width varchar,\n",
    "    transmission varchar,\n",
    "    valves_per_cylinder varchar,\n",
    "    wheelbase varchar,\n",
    "    dry_weight_kg float,\n",
    "    total_weight_kg float\n",
    ")\n",
    "\"\"\"\n",
    "cursor.execute(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec22ae9-3d12-4feb-a7b1-5234d83e345b",
   "metadata": {},
   "source": [
    "Con la tabla lista, ya podemos pasar a cargar los datos con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eba31065-e65a-454f-9b2c-70970700714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_HOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_PORT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_DB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_SCHEMA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_USER\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mREDSHIFT_CODER_PASSWORD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = transformed_df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:postgresql://{environ['REDSHIFT_CODER_HOST']}:{environ['REDSHIFT_CODER_PORT']}/{environ['REDSHIFT_CODER_DB']}\") \\\n",
    "    .option(\"dbtable\", f\"{environ['REDSHIFT_CODER_SCHEMA']}.{table_name}\") \\\n",
    "    .option(\"user\", environ['REDSHIFT_CODER_USER']) \\\n",
    "    .option(\"password\", environ['REDSHIFT_CODER_PASSWORD']) \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8857521-d45b-43a1-b141-7becf409396e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.10/site-packages (2.9.6)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (1.4.42)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy) (1.1.3.post0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "141c8eff-91ba-4b57-bfd5-52f4031327f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "conn = create_engine(f\"postgresql://{environ['REDSHIFT_CODER_HOST']}:{environ['REDSHIFT_CODER_PORT']}/{environ['REDSHIFT_CODER_DB']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41f6a38a-5e08-44e0-8853-e66a96247564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=environ['REDSHIFT_CODER_HOST'],\n",
    "    port=environ['REDSHIFT_CODER_PORT'],\n",
    "    dbname=environ['REDSHIFT_CODER_DB'],\n",
    "    user=environ['REDSHIFT_CODER_USER'],\n",
    "    password=environ['REDSHIFT_CODER_PASSWORD']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37c2b91c-8bbb-4591-96c3-a7bbbca99224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_to_write \u001b[38;5;241m=\u001b[39m \u001b[43mtransformed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_to_write\u001b[38;5;241m.\u001b[39mto_sql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREDSHIFT_CODER_SCHEMA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, conn, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    206\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    208\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:817\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 817\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_to_write = transformed_df.select(\"*\").toPandas()\n",
    "df_to_write.to_sql(f\"{environ['REDSHIFT_CODER_SCHEMA']}.{table_name}\", conn, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b059700-5283-4aa1-bca1-cf11042715e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
